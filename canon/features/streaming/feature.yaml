# Streaming Feature
# Extracted from code: src/streaming.lisp
# Status: Complete (code-only, now formalized)
# Confidence: 0.75

feature:
  name: streaming
  version: 1.0.0
  status: complete
  confidence: 0.75
  triangulation: code_only
  phase: 3

  description: |
    Real-time token-by-token LLM response display via streaming callbacks.
    Integrates with cl-llm-provider's streaming API to send response chunks
    to Emacs as they arrive, enabling live updates in the chat interface.

    Provides three callback factories (chunk, complete, error) that bridge
    cl-llm-provider streaming events to Emacs Lisp functions via eval-in-emacs.

metadata:
  primary_files:
    - src/streaming.lisp  # 70 lines

  specification:
    - specs/plans/2026-01-13-streaming-observability-upgrade.md  # Implementation plan

  tests:
    common_lisp: 0
    emacs_lisp: "partial (tested via chat integration)"
    coverage: "Tested via chat interface integration"

  dependencies:
    internal:
      - chat-interface  # Emacs side implements streaming display
    external:
      - cl-llm-provider  # Streaming API (:on-chunk, :on-complete, :on-error)

inventory:
  global_variables:
    - name: "*streaming-callback*"
      type: "function or nil"
      description: "Dynamic override for streaming callback"
      scope: "Can be bound per-request to customize streaming"

  functions:
    - name: make-emacs-streaming-callback
      type: callback-factory
      returns: lambda
      description: "Creates :on-chunk callback that sends deltas to Emacs"
      integration: "Calls agent-q--append-response-chunk in Emacs"

    - name: make-emacs-complete-callback
      type: callback-factory
      returns: lambda
      description: "Creates :on-complete callback for finalization"
      integration: "Calls agent-q--finalize-response + agent-q--update-token-usage"

    - name: make-emacs-error-callback
      type: callback-factory
      returns: lambda
      description: "Creates :on-error callback for error display"
      integration: "Calls agent-q--streaming-error in Emacs"

architecture:
  streaming_flow:
    1_request: "Agent calls cl-llm-provider:complete-stream with callbacks"
    2_chunks: "Provider calls :on-chunk for each token chunk"
    3_bridge: "Callback extracts delta, calls eval-in-emacs"
    4_emacs: "Emacs function appends chunk to chat buffer"
    5_complete: ":on-complete called with full content + usage"
    6_finalize: "Emacs finalizes display, updates token counts"

  error_handling:
    on_error: ":on-error callback formats condition, sends to Emacs"
    emacs_display: "agent-q--streaming-error shows error in chat"
    graceful_degradation: "Falls back to non-streaming on error (ADR-0002)"

  callback_integration:
    chunk_callback:
      input: "chunk object from cl-llm-provider"
      extraction: "cl-llm-provider:chunk-delta"
      output: "eval-in-emacs call with delta string"
      emacs_function: "agent-q--append-response-chunk"

    complete_callback:
      input: "full-content string, final-chunk object"
      extraction: "cl-llm-provider:chunk-usage"
      output: "Two eval-in-emacs calls (finalize + token update)"
      emacs_functions: ["agent-q--finalize-response", "agent-q--update-token-usage"]

    error_callback:
      input: "error condition"
      formatting: "format nil \"~A\""
      output: "eval-in-emacs call with error string"
      emacs_function: "agent-q--streaming-error"

contracts:
  count: 3
  list:
    - name: streaming-chunk-callback
      description: "Factory for :on-chunk callback (sends deltas to Emacs)"
      status: implicit

    - name: streaming-complete-callback
      description: "Factory for :on-complete callback (finalizes + tokens)"
      status: implicit

    - name: streaming-error-callback
      description: "Factory for :on-error callback (error display)"
      status: implicit

properties:
  count: 3
  list:
    - name: real-time-delivery
      description: "Chunks sent to Emacs immediately upon receipt"
      status: implicit

    - name: token-usage-reporting
      description: "Final token counts sent on completion"
      status: implicit

    - name: graceful-error-handling
      description: "Errors displayed, don't crash agent (ADR-0002)"
      status: implicit

scenarios:
  count: 3
  list:
    - name: successful-streaming-response
      description: "LLM streams tokens, Emacs displays incrementally"
      status: implicit

    - name: streaming-with-token-usage
      description: "Response completes, token counts updated in UI"
      status: implicit

    - name: streaming-error-recovery
      description: "Streaming fails, error displayed, can retry"
      status: implicit

quality_metrics:
  code_quality: excellent
  test_coverage: indirect (via chat integration)
  user_experience: excellent (real-time feedback)
  integration: excellent (clean callback pattern)

known_gaps:
  - "No formal contracts written"
  - "No formal properties documented"
  - "No formal scenarios"
  - "No direct unit tests (tested via chat integration)"
  - "*streaming-callback* override mechanism undocumented"
  - "No streaming progress indication (%, tokens, etc.)"
  - "No streaming cancellation support"
  - "No buffering/throttling for very fast streams"

related_features:
  - chat-interface  # Emacs side implements display
  - observability   # Streaming events could be logged

dependencies:
  - feature: chat-interface
    reason: "Emacs functions (agent-q--append-response-chunk, etc.) required"

  - external: cl-llm-provider
    reason: "Streaming API provides chunks"

dependents:
  - chat-interface  # Primary consumer of streaming

design_decisions:
  - decision: "eval-in-emacs for chunk delivery"
    rationale: "Direct RPC minimizes latency, no polling needed"

  - decision: "Separate callbacks for chunk/complete/error"
    rationale: "Matches cl-llm-provider API, clear separation of concerns"

  - decision: "Token usage in :on-complete"
    rationale: "Final usage not available until stream completes"

  - decision: "Graceful degradation (ADR-0002)"
    rationale: "Streaming failures shouldn't break agent"
    reference: "core/decisions/0002-streaming-tool-fallback.md"

notes: |
  This feature enables the real-time chat experience by bridging
  cl-llm-provider's streaming API to Emacs display functions.

  **Key insights**:
  1. **Callback factories**: Functions return lambdas (cl-llm-provider expects lambdas)
  2. **eval-in-emacs**: Direct RPC, no polling loop needed
  3. **Delta extraction**: chunk-delta gives incremental text
  4. **Usage delay**: Token counts only available at end

  **Integration flow**:
  ```
  Agent → cl-llm-provider:complete-stream
         → :on-chunk → make-emacs-streaming-callback
                     → eval-in-emacs(agent-q--append-response-chunk)
                                   → Chat buffer update
  ```

  **Confidence**: 0.75 (code-only, works well but lacks formal spec and tests)
